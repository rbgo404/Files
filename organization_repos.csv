Repository Name,Description,URL,Language,Stars,Forks,Created At,Updated At
Gptneo-pytorch,,https://github.com/inferless/Gptneo-pytorch,,0,0,2022-12-21T04:56:01Z,2024-09-16T21:51:55Z
Google-vit-tenserflow-template,,https://github.com/inferless/Google-vit-tenserflow-template,PureBasic,0,0,2022-12-21T06:21:16Z,2024-09-16T19:32:39Z
template-method,"GPT-Neo 125M is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 125M represents the number of parameters of this particular pre-trained model. Tag: <S3 Based>",https://github.com/inferless/template-method,Python,0,23,2023-05-09T05:02:06Z,2025-03-11T02:35:43Z
Facebook-bart-cnn,"BART model pre-trained on English language, and fine-tuned on CNN Daily Mail. It was introduced in the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Lewis et al. and first released in [this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart).",https://github.com/inferless/Facebook-bart-cnn,Python,8,3,2023-07-11T18:00:14Z,2025-04-01T17:36:30Z
vicuna-13b-8k,"A GPTQ‑quantized, 13‑billion‑parameter uncensored language model with an extended 8K context window, designed for dynamic, high‑performance conversational tasks. <metadata> gpu: T4 | collections: [""GPTQ""] </metadata>",https://github.com/inferless/vicuna-13b-8k,Python,0,1,2023-07-19T20:45:41Z,2025-03-25T07:03:25Z
Falcon-7b-template,Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets. It is made available under the Apache 2.0 license.,https://github.com/inferless/Falcon-7b-template,Python,0,0,2023-07-19T20:50:51Z,2025-02-03T21:49:18Z
stable-diffusion-controlnet,"This model with SD-ControlNet-Canny uses ControlNet with Canny edge maps to guide inpainting, producing consistent, detailed edits. <metadata> gpu: T4 | collections: [""Diffusers""] </metadata>",https://github.com/inferless/stable-diffusion-controlnet,Python,0,2,2023-07-19T20:52:44Z,2025-04-09T03:37:30Z
whisper-hindi-large,"Fine-tuned version of whisper-large-v2 on the Hindi data available from multiple publicly available ASR corpuses <metadata> gpu: T4 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/whisper-hindi-large,Python,0,2,2023-07-19T20:54:18Z,2025-03-25T06:51:28Z
universal-sentence-encoder-multilingual-tensorflow,"Google’s Universal Sentence Encoder Multilingual QA produces high-quality sentence embeddings optimized for cross-lingual question answering and semantic similarity tasks. <metadata> gpu: T4 | collections: [""Information Retrieval""] </metadata>",https://github.com/inferless/universal-sentence-encoder-multilingual-tensorflow,Python,0,3,2023-07-20T16:24:07Z,2025-03-25T06:50:53Z
dreamshaper,"A ControlNet model designed for Stable Diffusion, providing brightness adjustment for colorizing or recoloring images. <metadata> gpu: T4 | collections: [""Diffusers""] </metadata>",https://github.com/inferless/dreamshaper,Python,0,4,2023-07-26T18:35:56Z,2025-03-24T23:36:36Z
DINet,"A deformation inpainting network that enables realistic facial dubbing on high-resolution video by seamlessly modifying expressions with advanced inpainting techniques. <metadata> gpu: A100 | collections: [""Using Complex Outputs""] </metadata>",https://github.com/inferless/DINet,Python,0,1,2023-07-28T04:04:58Z,2025-04-14T07:04:36Z
llama-2-7b-hf,"A 7B parameter model fine-tuned for dialogue, utilizing supervised learning and RLHF, supports a context length of up to 4,000 tokens. <metadata> gpu: A10 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/llama-2-7b-hf,Python,1,3,2023-07-30T01:17:28Z,2025-03-25T07:03:13Z
Document-RAG-Upload,This is a semantic search application build using Inferless and Pinecone.,https://github.com/inferless/Document-RAG-Upload,Python,0,4,2023-08-02T17:16:03Z,2025-03-28T20:00:51Z
llama-2-7b-gptq,"A 7B conversational model fine-tuned with RLHF, deployable efficiently via vLLM for low-latency serving.  <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/llama-2-7b-gptq,Python,0,12,2023-08-08T15:52:09Z,2025-03-25T07:03:19Z
falcon-7b-instruct,"A 7B instruction-tuned language model that excels in following detailed prompts and effectively performing a wide variety of natural language processing tasks. <metadata> gpu: A100 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/falcon-7b-instruct,Python,0,2,2023-08-17T22:33:51Z,2025-03-25T07:03:09Z
Llama-2-13b-chat-AWQ,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.",https://github.com/inferless/Llama-2-13b-chat-AWQ,Python,0,3,2023-08-17T23:15:46Z,2025-03-11T02:29:04Z
vicuna-7b-8k,"A GPTQ‑quantized variant of Vicuna 7B v1.3, optimized for conversational AI and instruction‑following with efficient, robust performance. <metadata> gpu: T4 | collections:[""GPTQ""] </metadata>",https://github.com/inferless/vicuna-7b-8k,Python,0,2,2023-08-31T13:57:23Z,2025-03-25T07:03:05Z
vicuna-7b-1.1,"Open-source chatbot fine-tuned from LLaMA on 70K ShareGPT conversations, optimized for research and conversational tasks. <metadata> gpu: A100 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/vicuna-7b-1.1,Python,0,2,2023-08-31T17:04:08Z,2025-03-25T07:02:05Z
llama-2-13b-chat-hf,"A 13B model fine-tuned with reinforcement learning from human feedback, part of Meta’s Llama 2 family for dialogue tasks. <metadata> gpu: A100 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/llama-2-13b-chat-hf,Python,0,1,2023-09-14T22:19:41Z,2025-03-25T07:02:59Z
llama2-13b-8bit-gptq,"A quantized version of 13B fine-tuned model, optimized for dialogue use cases. <metadata> gpu: T4 | collections: [""HF Transformers"",""GPTQ""] </metadata>",https://github.com/inferless/llama2-13b-8bit-gptq,Python,0,3,2023-09-15T02:38:51Z,2025-04-14T16:23:35Z
hifi-gan-template,,https://github.com/inferless/hifi-gan-template,Python,0,0,2023-09-27T05:04:24Z,2025-03-11T02:35:32Z
stable-diffusion-2-inpainting,"An advanced text-guided inpainting model that fills masked image regions with contextually coherent, high-quality details. <metadata> gpu: T4 | collections: [""Diffusers""] </metadata>",https://github.com/inferless/stable-diffusion-2-inpainting,Python,2,2,2023-10-09T15:56:50Z,2025-03-25T17:57:07Z
idefics-9b-instruct-8bit,"IDEFICS (Image-aware Decoder Enhanced à la Flamingo with Interleaved Cross-attentionS) is an open-access reproduction of Flamingo, a closed-source visual language model developed by Deepmind. Like GPT-4, the multimodal model accepts arbitrary sequences of image and text inputs and produces text outputs.",https://github.com/inferless/idefics-9b-instruct-8bit,Python,0,3,2023-10-16T14:26:32Z,2025-04-13T14:44:29Z
stable-diffusion-v1-5,"A text-to-image model by Stability AI, renowned for generating high-quality, diverse images from text prompts.  <metadata> gpu: T4 | collections: [""Diffusers""] </metadata>",https://github.com/inferless/stable-diffusion-v1-5,Python,0,1,2023-10-31T09:15:57Z,2025-04-07T09:34:17Z
stabilityai-stable-diffusion-xl,"Generates high-quality images from text prompts using XL Refiner. <metadata> gpu: A100 | collections: [""Diffusers""] </metadata>",https://github.com/inferless/stabilityai-stable-diffusion-xl,Python,0,13,2023-10-31T15:50:56Z,2025-03-25T14:06:04Z
codellama-34b,"A 34B-parameter Python-specialized model for advanced code synthesis, completion, and understanding. <metadata> gpu: A100 | collections:[""AWQ""] </metadata>",https://github.com/inferless/codellama-34b,Python,0,5,2023-11-06T08:21:34Z,2025-03-25T06:50:00Z
s3-model-import,,https://github.com/inferless/s3-model-import,Python,0,0,2023-11-06T20:12:28Z,2025-03-11T02:35:17Z
stable-diffusion-webhook,Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.,https://github.com/inferless/stable-diffusion-webhook,Python,1,1,2023-11-14T07:09:53Z,2025-03-11T02:35:46Z
openai-whisper-large-v2,"Speech recognition model that achieves superior performance with improved noise resilience and multilingual support. <metadata> gpu: T4 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/openai-whisper-large-v2,Python,0,4,2023-11-21T08:22:59Z,2025-03-24T17:01:25Z
inferless_tutorials,,https://github.com/inferless/inferless_tutorials,Python,2,3,2023-11-24T12:26:47Z,2025-04-09T08:24:14Z
medcpt-query-encoder,"Generate embedding of biomedical short texts like questions, search queries and sentences. <metadata> gpu: T4 | collections: [""HF Transformers"",""Batch Input Processing""] </metadata>",https://github.com/inferless/medcpt-query-encoder,Python,0,0,2023-11-24T17:58:27Z,2025-03-25T06:50:39Z
medcpt-article-encoder,"Generates embeddings of biomedical articles that can be used for semantic search (dense retrieval). <metadata> gpu: T4 | collections: [""HF Transformers"",""Batch Input Processing""] </metadata>",https://github.com/inferless/medcpt-article-encoder,Python,0,0,2023-11-24T17:59:44Z,2025-03-25T06:50:30Z
stable-diffusion-s3-image-save,"Uses Stable Diffusion to generate images and automatically uploads them to an S3 bucket. <metadata> gpu: A100 | collections: [""S3 Storage"", ""Complex Outputs""] </metadata>",https://github.com/inferless/stable-diffusion-s3-image-save,Python,0,0,2023-11-27T11:04:11Z,2025-03-25T06:57:56Z
stable-diffusion-xl-turbo,"A distilled and cost-effective variant of SDXL that delivers high-quality text-to-image generation with accelerated inference speed. <metadata> gpu: T4 | collections: [""Diffusers""] </metadata>",https://github.com/inferless/stable-diffusion-xl-turbo,Python,3,10,2023-11-29T16:55:10Z,2025-04-07T10:05:47Z
ComfyUI,ComfyUI is a node-based GUI for Stable Diffusion. In this template we will import ComfyUI on Inferless.,https://github.com/inferless/ComfyUI,Python,1,5,2023-12-01T08:11:04Z,2025-02-10T19:27:04Z
bark,"Text-to-Speech model that generates realistic, multilingual speech with music, background noise, and sound effects. <metadata> gpu: T4 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/bark,Python,4,13,2023-12-20T09:20:56Z,2025-03-24T17:10:54Z
CodeLlama13b-with-adaptor,Code Llama is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 34 billion parameters. This is the repository for the 13 instruct-tuned version in the Hugging Face Transformers format. This model is designed for general code synthesis and understanding.,https://github.com/inferless/CodeLlama13b-with-adaptor,Python,0,3,2023-12-26T11:29:36Z,2025-03-11T02:28:16Z
model-onnx-runtime,,https://github.com/inferless/model-onnx-runtime,,0,0,2024-01-15T06:35:28Z,2024-09-16T21:51:06Z
dolphin-2.5-mixtral-8x7b-gptq,"A GPTQ‑quantized version of Eric Hartford’s Dolphin 2.5 Mixtral 8x7B model, fine‑tuned for coding and conversational tasks. <metadata> gpu: A100 | collections: [""vLLM"",""GPTQ""] </metadata>",https://github.com/inferless/dolphin-2.5-mixtral-8x7b-gptq,Python,5,10,2024-01-15T06:42:31Z,2025-03-25T07:01:51Z
Mixtral-echo,Tryecho's Mixtral-echo is a adapter for Mixtral model. The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks.,https://github.com/inferless/Mixtral-echo,Python,0,0,2024-01-17T09:37:31Z,2025-03-11T02:29:50Z
Mixral-8x7B,"Mixtral is a large language model developed by Mistral AI, a French artificial intelligence company. It is a sparse Mixture of Experts (MoE) model with 8 experts per MLP, totaling 45 billion parameters. Mixtral is designed to handle contexts of up to 32,000 tokens.",https://github.com/inferless/Mixral-8x7B,Python,0,0,2024-01-25T19:06:48Z,2025-04-14T12:26:52Z
CodeLlama-70B,,https://github.com/inferless/CodeLlama-70B,Python,0,1,2024-02-01T10:53:31Z,2025-03-11T02:28:09Z
translation-pipeline,,https://github.com/inferless/translation-pipeline,Python,0,0,2024-02-05T18:13:05Z,2025-03-11T02:28:19Z
speaker-pipeline,,https://github.com/inferless/speaker-pipeline,Python,0,0,2024-02-05T18:26:14Z,2025-03-11T02:29:44Z
Demuc-Pipeline,,https://github.com/inferless/Demuc-Pipeline,Python,0,0,2024-02-05T18:30:23Z,2025-03-11T02:35:24Z
mixtral-8x7b-v0.1,"A GPTQ-quantized variant of the Mixtral 8x7B model, fine-tuned for efficient text generation and conversational applications. <metadata> gpu: A100 | collections: [""vLLM"",""GPTQ""] </metadata>",https://github.com/inferless/mixtral-8x7b-v0.1,Python,2,5,2024-02-06T09:22:49Z,2025-03-25T07:01:45Z
stable-video-diffusion,"This model converts a single still image into a coherent video sequence with consistent, realistic motion. <metadata> gpu: T4 | collections: [""Diffusers""] </metadata>",https://github.com/inferless/stable-video-diffusion,Python,6,6,2024-02-06T09:28:06Z,2025-03-24T23:29:54Z
mistral-7b,"A 7B autoregressive language model by Mistral AI, optimized for efficient text generation and robust reasoning. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/mistral-7b,Python,3,13,2024-02-07T10:57:28Z,2025-03-25T07:01:33Z
meditron-7b-gptq,"An AWQ-quantized open-source medical LLM designed for exam question answering, differential diagnosis support, and providing comprehensive disease, symptom, cause, and treatment information. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/meditron-7b-gptq,Python,0,0,2024-02-07T12:36:11Z,2025-03-25T07:01:28Z
codellama-7b,"A 7B-parameter, Python-specialized model for lightweight, efficient code generation and comprehension. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/codellama-7b,Python,1,6,2024-02-07T12:52:36Z,2025-03-25T06:49:49Z
whisper-large-v3,"State‑of‑the‑art speech recognition model for English, delivering transcription accuracy across diverse audio scenarios. <metadata> gpu: T4 | collections: [""CTranslate2""] </metadata>",https://github.com/inferless/whisper-large-v3,Python,16,13,2024-02-07T12:55:08Z,2025-04-09T10:29:12Z
tenyxchat-7b,"A 7B chat model fine-tuned for robust conversational AI, delivering efficient, context-aware dialogue responses. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/tenyxchat-7b,Python,0,3,2024-02-07T12:56:56Z,2025-03-25T07:01:24Z
phi-2,"A small language model delivering robust text generation and instruction following with efficient long-context comprehension. <metadata> gpu: T4 | collections: [""vLLM"",""Batch Input Processing""] </metadata>",https://github.com/inferless/phi-2,Python,0,5,2024-02-07T12:58:28Z,2025-03-25T07:01:02Z
openhermes-2-5-mistral-7b,"A quantized model fine-tuned for rapid, efficient, and robust conversational and instruction tasks.  <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/openhermes-2-5-mistral-7b,Python,3,0,2024-02-07T13:00:52Z,2025-03-25T07:01:10Z
stable-diffusion-2-1,"Text-to-image model, refined for higher fidelity, improved prompt adherence, and diverse creative outputs. <metadata> gpu: T4 | collections: [""Diffusers""] </metadata>",https://github.com/inferless/stable-diffusion-2-1,Python,0,8,2024-02-07T13:02:16Z,2025-03-25T16:30:25Z
starling-lm-7b-alpha-gptq,"A GPTQ‑quantized 7B model optimized for efficient, high‑quality text generation across diverse tasks. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/starling-lm-7b-alpha-gptq,Python,0,1,2024-02-07T13:04:51Z,2025-03-25T07:00:58Z
DeciLM-7B,,https://github.com/inferless/DeciLM-7B,Python,0,1,2024-02-07T13:06:14Z,2025-04-14T13:25:53Z
TenyxChat-8x7B-v1,,https://github.com/inferless/TenyxChat-8x7B-v1,Python,0,0,2024-02-07T13:14:03Z,2025-04-12T05:55:42Z
inferless-docker-import-examples,,https://github.com/inferless/inferless-docker-import-examples,Python,0,2,2024-02-13T07:19:51Z,2024-02-13T07:20:04Z
Document-RAG-QnA,,https://github.com/inferless/Document-RAG-QnA,Python,0,0,2024-02-15T12:11:31Z,2025-03-11T02:29:14Z
Smaug-72B,"Smaug-72B - which topped the Hugging Face LLM leaderboard and it’s the first model with an average score of 80, making it the world’s best open-source foundation model.",https://github.com/inferless/Smaug-72B,Python,17,5,2024-02-17T21:01:38Z,2025-03-28T02:45:30Z
Gemma-7B,"Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. ",https://github.com/inferless/Gemma-7B,Python,1,2,2024-02-22T12:39:31Z,2025-03-11T01:59:48Z
gpt-neo-dynamic-batching,"Deploying a GPT-Neo model with Dynamic Batching where requests are dynamically batched. <metadata> collections: [""Dynamic Batching"",""HF Transformers""] </metadata>",https://github.com/inferless/gpt-neo-dynamic-batching,Python,0,0,2024-02-26T05:25:21Z,2025-03-25T07:00:52Z
llama-2-7b-chat,"AWQ quantized model offers significant memory savings and faster inference while maintaining strong conversational quality. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/llama-2-7b-chat,Python,0,2,2024-02-27T10:32:07Z,2025-03-25T07:03:59Z
LLava-1.5-13b,,https://github.com/inferless/LLava-1.5-13b,Python,0,0,2024-03-08T23:41:30Z,2025-03-11T02:35:28Z
Logo-Generator,,https://github.com/inferless/Logo-Generator,Python,2,6,2024-03-16T11:19:18Z,2025-03-28T19:54:35Z
LLaVA-1.6-34b,,https://github.com/inferless/LLaVA-1.6-34b,Python,1,0,2024-03-18T19:16:29Z,2025-02-03T21:32:10Z
stable-cascade,"A cascaded text-to-image diffusion model that sequentially refines outputs for enhanced detail, resolution, and overall image quality.  <metadata> gpu: T4 | collections: [""Diffusers""] </metadata>",https://github.com/inferless/stable-cascade,Python,0,1,2024-03-19T16:58:24Z,2025-03-25T13:05:33Z
TensorRT-LLM,,https://github.com/inferless/TensorRT-LLM,,10,1,2024-03-19T19:44:12Z,2025-03-22T16:26:34Z
Triton-vLLM-Backend,,https://github.com/inferless/Triton-vLLM-Backend,,0,0,2024-03-19T19:46:15Z,2024-03-19T19:46:16Z
sdxl-lightning,"A lightning-fast text-to-image generation model that generate high-quality 1024px images in a few steps. <metadata> gpu: T4 | collections: [""Diffusers""] </metadata>",https://github.com/inferless/sdxl-lightning,Python,3,6,2024-03-19T22:34:36Z,2025-03-24T22:38:55Z
Speech-to-Text-Whisper,,https://github.com/inferless/Speech-to-Text-Whisper,Python,0,0,2024-03-21T09:48:46Z,2025-03-11T02:35:51Z
mistral-7b-instruct-v0.2,"An 7B model with a 32k token context window and optimized attention mechanisms for superior dialogue and reasoning. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/mistral-7b-instruct-v0.2,Python,0,0,2024-03-21T10:54:39Z,2025-04-09T03:42:11Z
Voice-Conversational-Chatbot,,https://github.com/inferless/Voice-Conversational-Chatbot,Python,2,2,2024-03-21T11:41:20Z,2025-04-01T09:54:59Z
playground-v2.5,"Generate highly aesthetic 1024x1024 images with superior quality, flexible aspect ratios, and outstanding human preference alignment.  <metadata> gpu: T4 | collections: [""Diffusers""] </metadata>",https://github.com/inferless/playground-v2.5,Python,7,3,2024-03-21T20:23:41Z,2025-03-25T18:40:35Z
tinyllama-1.1b-chat-v1.0,"A chat model fine-tuned on TinyLlama, a compact 1.1B Llama model pretrained on 3 trillion tokens. <metadata> gpu: T4 | collections: [""vLLM""] </metadata>",https://github.com/inferless/tinyllama-1.1b-chat-v1.0,Python,1,2,2024-03-21T20:32:53Z,2025-03-25T07:13:26Z
zephyr-7b-beta,"A 7B fine-tuned model for instruction-following and context-aware text generation across a wide range of diverse applications. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/zephyr-7b-beta,Python,0,2,2024-03-21T20:42:07Z,2025-03-25T07:14:09Z
starcoder2-15b,"A 15B model trained on 600+ programming languages for high-quality code generation. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/starcoder2-15b,Python,0,2,2024-03-21T20:56:08Z,2025-03-11T02:32:59Z
animagine-xl-3.0,"High-quality image generation from text prompts, with improved hand anatomy and concept understanding.  <metadata> gpu: A10 | collections: [""Diffusers""] </metadata>",https://github.com/inferless/animagine-xl-3.0,Python,3,11,2024-03-24T00:14:44Z,2025-03-25T17:20:04Z
jina-embeddings-v2,"Text embedding model, delivering 768-dimensional embeddings and supporting up to 8192-token inputs. <metadata> gpu: T4 | collections: [""Information Retrieval""] </metadata>",https://github.com/inferless/jina-embeddings-v2,Python,2,7,2024-03-24T00:40:27Z,2025-03-29T14:51:52Z
gemma-2b-it,"2B instruct-tuned model for delivering coherent and instruction-following responses across a wide range of tasks. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/gemma-2b-it,Python,1,2,2024-03-24T01:33:13Z,2025-03-26T15:31:13Z
pyannote-speaker-diarization-3.1,"A state-of-the-art model that segments and labels audio recordings by accurately distinguishing different speakers. <metadata> gpu: T4 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/pyannote-speaker-diarization-3.1,Python,5,3,2024-03-24T22:06:18Z,2025-04-14T12:52:05Z
animagine-xl-3.1,"Generates high-quality anime images with improved hand anatomy and new aesthetic tags for enhanced image creation. <metadata> gpu: A10 | collections: [""Diffusers""] </metadata>",https://github.com/inferless/animagine-xl-3.1,Python,4,9,2024-03-27T12:50:41Z,2025-04-02T07:54:58Z
moondream2,"Tiny 1.93B vision-language model optimized for on-device image-to-text tasks and edge deployment. <metadata> gpu: T4 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/moondream2,Python,3,3,2024-03-27T18:41:41Z,2025-03-24T22:43:41Z
Command-r-v01,"35B model delivering high performance in reasoning, summarization, and question answering. <metadata> gpu: A100 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/Command-r-v01,Python,2,4,2024-03-27T21:50:59Z,2025-04-11T14:47:32Z
distil-whisper-large-v3,"Distilled model which is 49% smaller and 6.3× faster while maintaining near accuracy, especially on long-form transcription. <metadata> gpu: T4 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/distil-whisper-large-v3,Python,3,7,2024-03-27T22:20:27Z,2025-03-11T02:29:00Z
rmbg-1.4,"State-of-the-art background removal model, designed to effectively separate foreground from background. <metadata> gpu: T4 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/rmbg-1.4,Python,20,11,2024-03-28T03:37:22Z,2025-03-11T07:33:31Z
bge-m3,"A multi-lingual, versatile text embedding model supporting dense, sparse, and multi-vector retrieval. <metadata> gpu:T4 | collections: [""Information Retrieval""] </metadata>",https://github.com/inferless/bge-m3,Python,3,2,2024-03-28T11:44:37Z,2025-03-11T02:35:00Z
deepseek-coder-6.7b-instruct,"A 6.7B model fine-tuned on 2 billion tokens of instruction data, designed for code generation and completion tasks.  <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/deepseek-coder-6.7b-instruct,Python,1,6,2024-03-28T11:54:21Z,2025-03-11T02:34:17Z
moondream1,"1.6B model excels in responding to image-related queries. <metadata> gpu: T4 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/moondream1,Python,0,1,2024-03-28T13:52:55Z,2025-03-25T06:54:53Z
Jamba-v0.1,"Jamba is a state-of-the-art, hybrid SSM-Transformer LLM. It delivers throughput gains over traditional Transformer-based models, while outperforming or matching the leading models of its size class on most common benchmarks.",https://github.com/inferless/Jamba-v0.1,Python,0,0,2024-04-04T10:58:12Z,2025-01-15T19:35:32Z
neuralhermes-2.5-mistral-7b-gptq,"A GPTQ‑quantized 7B language model based on Mistral, fine‑tuned for robust, efficient conversational and text generation tasks. <metadata> gpu: A100 | collections: [""vLLM"",""GPTQ""] </metadata>",https://github.com/inferless/neuralhermes-2.5-mistral-7b-gptq,Python,0,2,2024-04-04T15:49:10Z,2025-03-25T07:13:54Z
openchat-3.5,"A fine-tuned chat model with C-RLFT - a strategy inspired by offline reinforcement learning, optimized for natural, context-aware conversations, excelling in instruction following and text generation tasks. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/openchat-3.5,Python,0,3,2024-04-04T16:23:55Z,2025-03-25T07:15:01Z
llama-2-13b-chat-gptq,"GPTQ quantized model, fine-tuned to delivers efficient dialogue performance and human-like responses. <metadata> gpu: A100 | collections: [""vLLM"",""GPTQ""] </metadata>",https://github.com/inferless/llama-2-13b-chat-gptq,Python,0,2,2024-04-04T16:33:31Z,2025-03-25T07:16:27Z
llama-2-70b-chat-gptq,"GPTQ quantized model fine-tuned for dialogue applications. <metadata> gpu: A100 | collections: [""vLLM"",""GPTQ""] </metadata>",https://github.com/inferless/llama-2-70b-chat-gptq,Python,0,2,2024-04-04T16:50:37Z,2025-03-11T02:33:17Z
neural-chat-7b-v3-1,"A fine-tuned 7B model based on mistralai/Mistral-7B-v0.1, aligned with DPO on Open-Orca/SlimOrca via Intel Gaudi 2, optimized for high-performance chat. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/neural-chat-7b-v3-1,Python,0,1,2024-04-04T17:02:00Z,2025-03-25T07:15:16Z
ms-marco-minilm-l-12-v2,"Fine-tuned on MS MARCO for efficient dense sentence embeddings, excelling in semantic search and retrieval. <metadata> gpu: T4 | collections:[""Information Retrieval""] </metadata>",https://github.com/inferless/ms-marco-minilm-l-12-v2,Python,0,2,2024-04-04T18:38:03Z,2025-03-11T02:31:12Z
distil-whisper-large-v2,"Distilled Whisper model that is 6 times faster, 49% smaller, and performs within 1% WER on out-of-distribution evaluation sets. <metadata> gpu: T4 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/distil-whisper-large-v2,Python,1,3,2024-04-05T19:30:21Z,2025-03-11T02:31:29Z
sam,"Produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image. <metadata> gpu: T4 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/sam,Python,2,4,2024-04-06T22:35:11Z,2025-03-25T06:06:13Z
multilingual-e5-large,"An embedding model, trained on a mixture of multilingual datasets and supports 100 languages. <metadata> gpu: T4 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/multilingual-e5-large,Python,3,2,2024-04-07T09:28:04Z,2025-03-24T23:34:37Z
Musicgen-stereo-melody-large,,https://github.com/inferless/Musicgen-stereo-melody-large,Python,1,1,2024-04-16T20:23:47Z,2025-04-10T10:41:52Z
llama-3,"A robust 8B parameter base model for diverse language tasks, offering strong performance in multilingual scenarios. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/llama-3,Python,2,8,2024-04-24T06:11:12Z,2025-03-11T02:33:21Z
Llama3-TenyxChat-70B,,https://github.com/inferless/Llama3-TenyxChat-70B,Python,1,2,2024-04-30T08:03:16Z,2025-03-17T06:24:43Z
melo-tts,A high-quality text-to-speech model by MyShell.ai that supports multiple English accents and real-time inference.,https://github.com/inferless/melo-tts,Python,0,1,2024-04-30T12:56:12Z,2025-04-08T04:38:55Z
YouTube-Video-Summarizer,,https://github.com/inferless/YouTube-Video-Summarizer,Python,0,1,2024-05-03T13:38:09Z,2025-03-11T02:28:02Z
Customer-Service-Voicebot,,https://github.com/inferless/Customer-Service-Voicebot,Python,2,2,2024-05-15T06:30:55Z,2025-04-06T17:17:58Z
google-paligemma-3b,"Vision-language model combining SigLIP and Gemma, fine-tuned on diverse tasks to generate text from image-text inputs. <metadata> gpu: T4 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/google-paligemma-3b,Python,4,3,2024-05-20T08:23:59Z,2025-03-13T13:55:04Z
timesfm,A pretrained time-series foundation model developed by Google Research for time-series forecasting. <metadata> gpu: A100 </metadata>,https://github.com/inferless/timesfm,Python,2,6,2024-05-21T08:22:37Z,2025-03-24T20:52:10Z
phi-3-128k,"An instruction-tuned mini LLM with a 128k token context window, enabling efficient long-context comprehension and generation. <metadata> gpu: T4 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/phi-3-128k,Python,0,6,2024-05-24T18:52:18Z,2025-03-24T15:13:38Z
Flan-UL2,,https://github.com/inferless/Flan-UL2,Python,0,1,2024-05-31T09:08:09Z,2025-03-11T02:29:21Z
donut-doc-vqa,"An OCR-free document understanding model that uses a Swin Transformer encoder and BART decoder, fine-tuned on the DocVQA dataset.",https://github.com/inferless/donut-doc-vqa,Python,0,1,2024-05-31T11:40:00Z,2025-04-08T04:38:41Z
zephyr-7b-streaming,"Zephyr-7B model with Server-Sent Events (SSE) enables real-time streaming for chat based applications. <metadata> gpu: A100 | collections: [""Streaming LLMs"", ""SSE Events""] </metadata>",https://github.com/inferless/zephyr-7b-streaming,Python,0,1,2024-06-10T05:16:45Z,2025-03-25T07:12:42Z
realvis-xl_v4.0_lightning,"A lightweight, accelerated variant of RealVisXL V4.0, engineered for real‑time, high‑quality image generation with enhanced efficiency.  <metadata> gpu: T4 | collections: [""Diffusers""] </metadata>",https://github.com/inferless/realvis-xl_v4.0_lightning,Python,0,6,2024-06-11T20:47:30Z,2025-04-10T20:48:48Z
qwen2-72b-instruct,"A 72B instruct-tuned language model, AWQ-quantized for efficient inference and robust performance on diverse instruction tasks. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/qwen2-72b-instruct,Python,0,3,2024-06-21T11:33:29Z,2025-04-08T04:39:16Z
stable-diffusion-3,"A latent diffusion model fine-tuned on diverse image–text pairs, balancing quality and speed.  <metadata> gpu: A10 | collections: [""Diffusers""] </metadata>",https://github.com/inferless/stable-diffusion-3,Python,0,4,2024-06-21T13:51:03Z,2025-03-25T15:58:06Z
triton-co-pilot,Generate Glue Code in seconds to simplify your Nvidia Triton Inference Server Deployments,https://github.com/inferless/triton-co-pilot,Python,19,3,2024-07-01T21:35:48Z,2025-02-12T08:31:41Z
codellama-34b-vllm,"A 34B-parameter, Python-specialized model for advanced code synthesis, deploy with vLLM for efficient inference. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/codellama-34b-vllm,Python,0,5,2024-07-11T18:22:12Z,2025-03-11T02:32:56Z
BLIP-2,"BLIP-2 is an Vision Language model which consists of 3 models: a CLIP-like image encoder, a Querying Transformer (Q-Former) and a large language model. It excels in image-text interactions, offering capabilities like image captioning and visual question answering.",https://github.com/inferless/BLIP-2,,0,0,2024-07-15T11:36:46Z,2024-09-23T16:30:06Z
parler-tts-streaming,"text-to-speech with Server-Sent Events (SSE) streams real-time audio for chat-based applications. <metadata> gpu: A100 | collections: [""SSE Events""] </metadata>",https://github.com/inferless/parler-tts-streaming,Python,2,10,2024-07-17T08:46:05Z,2025-03-11T02:35:04Z
Llama-3-peft-model,,https://github.com/inferless/Llama-3-peft-model,Python,0,0,2024-07-18T13:28:02Z,2024-07-18T16:08:14Z
Whisper-asr-streaming,,https://github.com/inferless/Whisper-asr-streaming,,0,0,2024-07-22T12:29:45Z,2024-07-22T12:29:49Z
CodeFormer,,https://github.com/inferless/CodeFormer,,0,0,2024-07-23T06:09:37Z,2024-07-23T12:59:51Z
llama-3.1-8b-instruct,"An 8B multilingual instruction model fine-tuned with RLHF for chat completion, supporting up to 128k tokens. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/llama-3.1-8b-instruct,Python,0,6,2024-07-24T08:52:07Z,2025-03-25T06:22:08Z
llama-3.1-8b-instruct-gguf," An 8B-parameter, instruction-tuned variant of Meta's Llama-3.1 model, optimized in GGUF format for efficient inference. <metadata> gpu: A100 | collections: [""lama.cpp""] </metadata>",https://github.com/inferless/llama-3.1-8b-instruct-gguf,Python,0,7,2024-08-02T07:14:14Z,2025-03-25T04:03:17Z
SAM-2,,https://github.com/inferless/SAM-2,,0,0,2024-08-03T13:47:02Z,2024-08-03T14:08:03Z
bge-base-en-v1.5,,https://github.com/inferless/bge-base-en-v1.5,Python,0,2,2024-08-05T18:48:38Z,2025-03-11T02:28:13Z
llama-2-7b-chat-gguf,"Quantized GGUF model which dramatically reduces memory requirements while preserving conversational quality. <metadata> gpu: A100 | collections: [""Using NFS Volumes"", ""llama.cpp""] </metadata>",https://github.com/inferless/llama-2-7b-chat-gguf,Python,0,1,2024-08-06T13:39:17Z,2025-04-14T13:45:46Z
llama-2-docker-template,"Deploy Llama-2 model with TGI on Inferless though Dockerfile. <metadata> collections: [""Dockerfile"", ""FastAPI Containers""] </metadata>",https://github.com/inferless/llama-2-docker-template,Python,0,1,2024-08-08T17:29:09Z,2025-02-28T09:03:36Z
tgi-template,"Import any model using TGI on Inferless using Dockerfile for streamlined containerized deployment. <metadata> collections: [""Dockerfile""] </metadata>",https://github.com/inferless/tgi-template,Dockerfile,0,4,2024-08-11T09:52:40Z,2025-02-28T09:03:50Z
tinyllama-1.1b-chat-vllm-gguf,"Deploy GGUF quantized version of Tinyllama-1.1B GGUF vLLM for efficient inference. <metadata> gpu: A100 | collections: [""Using NFS Volumes"", ""vLLM""] </metadata>",https://github.com/inferless/tinyllama-1.1b-chat-vllm-gguf,Python,1,7,2024-08-13T10:25:13Z,2025-04-10T17:17:22Z
flux.1-schnell,"12B text-to-image mode that upscales and generates high-quality images from text prompts using advanced diffusion techniques. <metadata> gpu: A100 | collections: [""Diffusers"",""Variable Inputs""] </metadata>",https://github.com/inferless/flux.1-schnell,Python,4,11,2024-08-20T12:10:22Z,2025-03-25T06:17:07Z
Multi_LoRA_Adapter,,https://github.com/inferless/Multi_LoRA_Adapter,Python,0,1,2024-08-23T00:01:20Z,2025-03-11T02:27:59Z
gemma-2-9b-it,"Instruct-tuned model for instruction following, delivering coherent, high-quality responses across a broad spectrum of tasks. <metadata> gpu: A10 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/gemma-2-9b-it,Python,0,2,2024-08-24T04:33:59Z,2025-03-25T20:47:20Z
ComfyUI-template-new,,https://github.com/inferless/ComfyUI-template-new,Python,0,0,2024-09-01T03:09:53Z,2025-02-03T21:54:39Z
ComfyUI-Inferless-template,,https://github.com/inferless/ComfyUI-Inferless-template,Python,0,3,2024-09-06T10:18:11Z,2025-02-10T19:28:33Z
Llama-3.1-70B-awq,,https://github.com/inferless/Llama-3.1-70B-awq,Python,0,2,2024-09-11T17:19:04Z,2025-03-11T02:29:31Z
SOLAR-10.7B-Instruct,,https://github.com/inferless/SOLAR-10.7B-Instruct,Python,0,0,2024-09-11T18:20:44Z,2025-03-11T02:29:46Z
Blip-image-captioning-large,The Salesforce/blip-image-captioning-large model is a image captioning model developed as part of the BLIP (Bootstrapping Language-Image Pre-training) framework. ,https://github.com/inferless/Blip-image-captioning-large,,0,0,2024-09-20T12:43:18Z,2024-09-23T16:29:50Z
InternVL2-Llama3-76B-AWQ,,https://github.com/inferless/InternVL2-Llama3-76B-AWQ,Python,0,1,2024-09-21T16:51:49Z,2025-04-10T20:54:35Z
phi-3.5-moe-instruct,"An instruction-tuned variant of Phi-3.5, delivering efficient, context-aware responses across diverse language tasks.  <metadata> gpu: A100 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/phi-3.5-moe-instruct,Python,0,1,2024-09-25T22:31:45Z,2025-03-25T07:04:45Z
Phi-3.5-MoE-instruct-8bit,"Phi-3.5-MoE a compact yet powerful model designed for instruction-following tasks. This model is part of the Phi-3 family, known for its efficiency and high performance. The Phi-3 Mini-128K-Instruct exhibited robust, state-of-the-art performance among models with fewer than 13B parameters.",https://github.com/inferless/Phi-3.5-MoE-instruct-8bit,Python,0,0,2024-09-25T23:48:07Z,2025-04-13T16:09:40Z
Llama-server-template,,https://github.com/inferless/Llama-server-template,Python,0,0,2024-09-26T12:09:53Z,2024-09-26T19:52:17Z
llama-3.2-11b-vision-instruct,"11B multimodal model integrating vision and text for image reasoning, captioning, and Q&A. <metadata> gpu: A100 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/llama-3.2-11b-vision-instruct,Python,7,13,2024-09-26T16:08:48Z,2025-03-11T02:32:05Z
bark-streaming,"Bark text-to-speech with Server-Sent Events (SSE) streams real-time audio. <metadata> gpu: A100 | collections: [""SSE Events""] </metadata>",https://github.com/inferless/bark-streaming,Python,0,3,2024-10-02T22:19:30Z,2025-03-25T06:47:26Z
whisper-large-v3-turbo,"A turbocharged variant of Whisper large‑v3 for English speech recognition, optimized for lower latency. <metadata> gpu: T4 | collections: [""HF Transformers"",""Complex Outputs""] </metadata>",https://github.com/inferless/whisper-large-v3-turbo,Python,0,7,2024-10-09T08:01:33Z,2025-03-11T02:32:25Z
ministral-8b-instruct,"An 8B instruction-tuned model optimized for generating coherent, context-rich responses across diverse applications. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/ministral-8b-instruct,Python,0,0,2024-10-24T10:09:37Z,2025-03-25T07:18:44Z
flux-upscaler-xs,"A ControlNet model from Jasper AI that enhances low-resolution images by upscaling them with advanced diffusion techniques. <metadata> gpu: A100 | collections: [""Diffusers"",""Variable Inputs""] </metadata>",https://github.com/inferless/flux-upscaler-xs,Python,0,3,2024-11-04T12:58:22Z,2025-03-11T02:31:05Z
Book-Audio-Summary-Generator,,https://github.com/inferless/Book-Audio-Summary-Generator,Python,0,0,2024-11-07T22:09:31Z,2025-04-12T17:14:55Z
qwen2.5-coder-32b-instruct,"A State-Of-The-Art coder LLM, tailored for instruction-based tasks, particularly in code generation, reasoning, and repair.  <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/qwen2.5-coder-32b-instruct,Python,1,3,2024-11-12T10:41:04Z,2025-03-11T02:34:40Z
Financial-AI-Agent,,https://github.com/inferless/Financial-AI-Agent,,0,0,2024-11-18T16:14:39Z,2025-01-15T03:45:15Z
qwen2-vl-7b-instruct,"A 7B multimodal language model designed for instruction-based tasks with advanced visual and multilingual capabilities. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/qwen2-vl-7b-instruct,Python,0,10,2024-11-28T10:31:58Z,2025-03-24T21:31:14Z
Qwen2-VL-7B-Instruct-pydantic,,https://github.com/inferless/Qwen2-VL-7B-Instruct-pydantic,Python,0,0,2024-12-03T09:39:27Z,2024-12-05T12:06:59Z
FluxUpscalerXS-pydantic,,https://github.com/inferless/FluxUpscalerXS-pydantic,Python,0,0,2024-12-05T12:35:52Z,2024-12-05T18:58:24Z
Qwen2.5-Coder-32B-Instruct-pydantic,,https://github.com/inferless/Qwen2.5-Coder-32B-Instruct-pydantic,Python,0,0,2024-12-05T19:00:04Z,2024-12-05T19:00:32Z
qwq-32b-preview,"A 32B experimental reasoning model for advanced text generation and robust instruction following.  <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/qwq-32b-preview,Python,17,6,2024-12-09T15:04:35Z,2025-04-13T16:50:37Z
TRELLIS,,https://github.com/inferless/TRELLIS,Python,0,3,2024-12-17T07:10:00Z,2025-03-11T02:28:32Z
Trellis-Inferless,,https://github.com/inferless/Trellis-Inferless,Python,0,1,2024-12-30T07:09:42Z,2024-12-31T14:10:30Z
huatuogpt-o1-70b,"A medical LLM built on LLaMA-3.1-70B, employing detailed step-by-step reasoning for complex medical problem-solving.  <metadata> gpu: A100 | collections: [""HF Transformers"",""Variable Inputs""] </metadata>",https://github.com/inferless/huatuogpt-o1-70b,Python,0,0,2025-01-10T04:47:13Z,2025-03-25T07:04:24Z
phi-4,"A 14B model designed to excel in complex reasoning tasks, particularly within STEM domains. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/phi-4,Python,0,5,2025-01-15T09:54:52Z,2025-03-11T02:29:53Z
deepseek-r1-distill-qwen-32b,"A distilled DeepSeek-R1 variant built on Qwen2.5-32B, fine-tuned with curated data for enhanced performance and efficiency. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/deepseek-r1-distill-qwen-32b,Python,16,23,2025-01-27T05:27:44Z,2025-03-29T12:04:14Z
AI-Explainer,,https://github.com/inferless/AI-Explainer,Python,0,0,2025-01-27T17:25:29Z,2025-01-27T18:59:58Z
NotebookLM,,https://github.com/inferless/NotebookLM,,0,0,2025-02-01T11:31:37Z,2025-02-05T14:51:22Z
qwen2.5-vl-7b-instruct,"Vision-Language model that integrates advanced image, video, and text understanding. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/qwen2.5-vl-7b-instruct,Python,0,4,2025-02-07T12:17:24Z,2025-03-11T02:34:47Z
Stable-Diffusion-3.5-large,,https://github.com/inferless/Stable-Diffusion-3.5-large,Python,0,0,2025-02-11T16:33:56Z,2025-04-10T20:50:49Z
Stable-Diffusion-3.5-large-turbo,,https://github.com/inferless/Stable-Diffusion-3.5-large-turbo,Python,0,1,2025-02-12T03:02:27Z,2025-03-11T02:29:07Z
mistral-small-24b-instruct,"24B instruction-tuned model, delivering context-aware, reliable responses optimized for performance and efficiency. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/mistral-small-24b-instruct,Python,0,3,2025-02-12T03:07:31Z,2025-03-11T02:34:06Z
mistral-7b-instruct-v0.3,"7B model fine-tuned for precise instruction following and robust contextual understanding. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/mistral-7b-instruct-v0.3,Python,0,2,2025-02-12T04:03:44Z,2025-03-11T02:32:01Z
llama-3.2-3b-instruct,"3B compact instruction-tuned model generate detailed responses across a range of tasks. <metadata> gpu: A100 | collections: [""vLLM""] </metadata>",https://github.com/inferless/llama-3.2-3b-instruct,Python,0,5,2025-02-12T19:48:19Z,2025-03-11T02:34:23Z
Zyphra-Zonos,,https://github.com/inferless/Zyphra-Zonos,,0,0,2025-02-18T16:48:09Z,2025-02-18T16:48:13Z
phi-4-GGUF,"A 14B model optimized in GGUF format for efficient inference, designed to excel in complex reasoning tasks. <metadata> gpu: A100 | collections: [""llama.cpp"",""GGUF""] </metadata>",https://github.com/inferless/phi-4-GGUF,Python,0,6,2025-02-24T18:24:11Z,2025-03-25T07:03:33Z
phi4-vllm-gguf,"A 14B model optimized in GGUF format for efficient inference, designed to excel in complex reasoning tasks. <metadata> gpu: A100 | collections: [""vLLM"",""GGUF""] </metadata>",https://github.com/inferless/phi4-vllm-gguf,Python,0,1,2025-03-05T04:27:05Z,2025-04-08T04:28:32Z
phi4-vllm-gptq,"A 14B model optimized in GPTQ format for efficient inference, designed to excel in complex reasoning tasks. <metadata> gpu: A100 | collections: [""vLLM"",""GPTQ""] </metadata>",https://github.com/inferless/phi4-vllm-gptq,Python,0,2,2025-03-05T06:19:08Z,2025-03-25T07:03:44Z
phi-4-multimodal-instruct,"State‑of‑the‑art multimodal foundation model developed by Microsoft Research which seamlessly fuses robust language understanding with advanced visual and audio analysis. <metadata> gpu: A100 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/phi-4-multimodal-instruct,Python,0,4,2025-03-09T09:29:04Z,2025-03-24T01:33:44Z
gemma-3-27b-it,"Gemma-3-27B-it is a multimodal model that handles both text and image inputs, supports over 140 languages, and features a context window of up to 128,000 tokens. <metadata> gpu: A100 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/gemma-3-27b-it,Python,0,8,2025-03-17T11:36:59Z,2025-03-28T04:33:54Z
product-hunt-thread-summarizer,,https://github.com/inferless/product-hunt-thread-summarizer,Python,0,0,2025-03-19T13:44:25Z,2025-03-21T04:19:33Z
mistral-small-3.1-24b-instruct,"Advanced multimodal language model developed by Mistral AI with enhanced text performance, robust vision capabilities, and an expanded context window of up to 128,000 tokens. <metadata> gpu: A100 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/mistral-small-3.1-24b-instruct,Python,1,7,2025-03-20T17:13:02Z,2025-04-04T16:13:53Z
template,Using Template for Model Deployment ,https://github.com/inferless/template,Python,0,21,2025-03-20T21:25:10Z,2025-03-24T06:06:46Z
spatiallm-llama-1b,"A 3D large language model that processes point cloud data to produce structured 3D scene representations. <metadata> gpu: A100 | collections: [""HF Transformers""] </metadata>",https://github.com/inferless/spatiallm-llama-1b,Python,1,3,2025-03-28T09:13:20Z,2025-04-02T16:32:30Z
spatiallm-qwen-0.5b,,https://github.com/inferless/spatiallm-qwen-0.5b,Python,0,0,2025-03-28T11:38:24Z,2025-03-28T11:40:44Z
hibou-l,AutoImageProcessorTemplate,https://github.com/inferless/hibou-l,Python,0,0,2025-03-31T06:37:22Z,2025-04-01T11:46:54Z
